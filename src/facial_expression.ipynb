{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K \n",
    "\n",
    "from scipy.stats import chisquare\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "data_file= \"../input/facial-expression/fer2013/fer2013.csv\"\n",
    "data= pd.read_csv(data_file)\n",
    "\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Chi-Squared with Uniform Distribution\n",
    "target_OHE= pd.get_dummies(data.emotion, prefix='emotion')\n",
    "target_count=target_OHE.sum()\n",
    "print(target_count)\n",
    "target_count.plot(kind='bar')\n",
    "\n",
    "print(\"Chi-Squared for initial Dataset\")\n",
    "chisquare(target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Subset for Hyperopt\n",
    "ss_data= pd.DataFrame()\n",
    "\n",
    "for i in range(0, 7):\n",
    "    if i in [2, 3, 4, 6]:\n",
    "        ss_data= ss_data.append(data.loc[data.emotion==i])\n",
    "        \n",
    "#    if i != 1:\n",
    "#        ss_data= ss_data.append(data.loc[data.emotion==i].sample(3500))\n",
    "        \n",
    "#    if i==1:\n",
    "#        ss_data=ss_data.append(data.loc[data.emotion==1].sample(547))    \n",
    "        \n",
    "data= ss_data\n",
    "data= data.replace(2, 0)\n",
    "data= data.replace(3, 1)\n",
    "data= data.replace(4, 2)\n",
    "data= data.replace(6, 3)\n",
    "\n",
    "#Chi-Squared\n",
    "target_OHE= pd.get_dummies(data.emotion, prefix='emotion')\n",
    "target_count=target_OHE.sum()\n",
    "print(target_count)\n",
    "target_count.plot(kind='bar')\n",
    "\n",
    "print(\"Chi-Squared for Dataset with 4 Classes\")\n",
    "chisquare(target_count) #f_exp to default/equally likely\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Converting Data into Tensorflow proprietary Format\n",
    "temp_gran_pixels= data['pixels'].str.split(\" \", expand= True)\n",
    "temp_gran_pixels= temp_gran_pixels.replace('%','',regex=True).astype('float')/255\n",
    "\n",
    "data= data.join(temp_gran_pixels, how= 'left')\n",
    "data= data.drop(columns=['pixels', 'Usage'])\n",
    "\n",
    "df_train_features, df_test_features, df_train_label, df_test_label= \\\n",
    "    train_test_split(data.copy().drop(columns=['emotion']), data.emotion, test_size=0.15, random_state=42, shuffle= True)\n",
    "\n",
    "df_train_features= np.array(df_train_features).copy()\n",
    "df_test_features= np.array(df_test_features).copy()\n",
    "df_train_label= np.array(df_train_label).copy()\n",
    "df_test_label= np.array(df_test_label).copy()\n",
    "\n",
    "df_train_features= df_train_features.reshape(df_train_features.shape[0], 48, 48, 1)\n",
    "df_test_features= df_test_features.reshape(df_test_features.shape[0], 48, 48, 1)\n",
    "\n",
    "del data\n",
    "del data_file\n",
    "del target_OHE\n",
    "gc.collect() \n",
    "\n",
    "#One-Hot-Encode Labels\n",
    "df_train_label= tf.keras.utils.to_categorical(df_train_label)\n",
    "df_test_label= tf.keras.utils.to_categorical(df_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_two = tf.keras.Sequential()  \n",
    "\n",
    "#INPUT\n",
    "model_two.add(tf.keras.layers.Conv2D(32, (3, 3), padding= 'SAME', strides=(1, 1), use_bias= True, kernel_regularizer=keras.regularizers.l2(0.0001), input_shape=(48, 48, 1))) #48, 48, 1\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model_two.add(tf.keras.layers.MaxPooling2D((2, 2), padding= 'SAME'))\n",
    "\n",
    "#BETWEEN\n",
    "#No Dropout after Input-Layer model_two.add(tf.keras.layers.Dropout(rate=0.2)) #lower probability of getting disconnected after input layer\n",
    "\n",
    "#HIDDEN 1\n",
    "model_two.add(tf.keras.layers.Conv2D(64, (3, 3), padding= 'SAME', use_bias= True, kernel_regularizer=keras.regularizers.l2(0.0001), strides=(1, 1)))\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model_two.add(tf.keras.layers.MaxPooling2D((2, 2), padding= 'SAME'))\n",
    "\n",
    "#BETWEEN\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "#HIDDEN 2\n",
    "model_two.add(tf.keras.layers.Conv2D(128, (3, 3), padding= 'SAME', use_bias= True, kernel_regularizer=keras.regularizers.l2(0.0001), strides=(1, 1)))\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model_two.add(tf.keras.layers.MaxPooling2D((2, 2), padding= 'SAME'))\n",
    "\n",
    "#BETWEEN\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "#OUTPUT\n",
    "model_two.add(tf.keras.layers.Flatten())\n",
    "model_two.add(tf.keras.layers.Dense(32))\n",
    "model_two.add(tf.keras.layers.BatchNormalization())\n",
    "model_two.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model_two.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model_two.add(tf.keras.layers.Dense(7, activation= tf.nn.softmax))\n",
    "\n",
    "model_two.summary()\n",
    "\n",
    "model_two.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#Train theModel\n",
    "EPOCHS=8\n",
    "BATCH_SIZE=128\n",
    "\n",
    "history= model_two.fit(df_train_features, \n",
    "                       df_train_label, \n",
    "                       epochs= EPOCHS, \n",
    "                       batch_size= BATCH_SIZE,\n",
    "                       validation_split= 0.18, #(0.15/0.85) #makes comparision of epochs more efficient\n",
    "                       shuffle=True)\n",
    "\n",
    "print('\\nhistory dict:', history.history)\n",
    "\n",
    "#Export Model\n",
    "model_json = model_two.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model_two.save_weights(\"model.h5\")\n",
    "print(\"Saved model\")\n",
    "\n",
    "test_loss, test_accuracy= model_two.evaluate(df_test_features, df_test_label)\n",
    "print(\"Test Loss= \" + str(test_loss))\n",
    "print(\"Test Accuracy= \" + str(test_accuracy))\n",
    "\n",
    "\n",
    "# Accuracy per Epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# Loss per Epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter-Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import numpy as np\n",
    "\n",
    "filters= [32, 64, 128, 256, 512]\n",
    "\n",
    "def optimize(params):\n",
    "    #This is the optimization function that given a space (space here) of \n",
    "    #hyperparameters and a scoring function (score here), finds the best hyperparameters.\n",
    "\n",
    "    # To learn more about XGBoost parameters, head to this page: \n",
    "    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    space = params\n",
    "    \n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters (min, cause score is 1-accuracy)\n",
    "    best = fmin(score, \n",
    "                space, \n",
    "                algo=tpe.suggest, \n",
    "                max_evals=15\n",
    "                #max_evals=50\n",
    "               )\n",
    "    \n",
    "    return best\n",
    "\n",
    "def score(params):\n",
    "    print(params)\n",
    "    temp_model= build_model(params)\n",
    "    \n",
    "    history= temp_model.fit(df_train_features, \n",
    "                            df_train_label, \n",
    "                            #epochs= 1,\n",
    "                            epochs= int(params[\"hp_EPOCHS\"]), \n",
    "                            batch_size= int(params[\"hp_BATCH_SIZE\"]),\n",
    "                            #validation_split= 0.999,\n",
    "                            validation_split= 0.18, #(0.15/0.85) #makes comparision of epochs more efficient\n",
    "                            #validation_data= (df_test_features, df_test_label), #display val\n",
    "                            shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\nhistory dict:', history.history)\n",
    "    \n",
    "    score= history.history['val_accuracy']\n",
    "    score= score[0]\n",
    "    loss= 1-float(score)\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "def build_model(params):\n",
    "    model_hype = tf.keras.Sequential()\n",
    "    \n",
    "    #INPUT & HIDDEN\n",
    "    for i in range(0, (int(params[\"hp_HIDDEN_LAYERS\"]) +1)):\n",
    "                  model_hype= build_layer(model_hype, params, i)\n",
    "                  \n",
    "                  if i != 0:\n",
    "                      model_hype= build_between(model_hype, params)\n",
    "                  \n",
    "    #OUTPUT\n",
    "    model_hype= build_output(model_hype, params)\n",
    "    model_hype.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(learning_rate= params[\"hp_LEARNING_RATE\"]),\n",
    "                  #learning_rate=params[\"hp_LEARNING_RATE\"],\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model_hype.summary()\n",
    "    \n",
    "    return model_hype\n",
    "\n",
    "def build_layer(model, params, nth_layer):\n",
    "    if nth_layer== 0:\n",
    "        model.add(tf.keras.layers.Conv2D(#params[\"hp_FILTERS\"][nth_layer], \n",
    "                                         filters[nth_layer],\n",
    "                                         params[\"hp_KERNEL_SIZE\"], \n",
    "                                         padding= params[\"hp_PADDING\"], \n",
    "                                         strides=params[\"hp_STRIDES\"], \n",
    "                                         use_bias= params[\"hp_USE_BIAS\"], \n",
    "                                         kernel_regularizer=keras.regularizers.l2(params[\"hp_KERNEL_REGULARIZER\"]),\n",
    "                                         input_shape=(48, 48, 1))) #48, 48, 1\n",
    "                  \n",
    "    if nth_layer!= 0:   \n",
    "        model.add(tf.keras.layers.Conv2D(#params[\"hp_FILTERS\"][nth_layer], \n",
    "                                         filters[nth_layer],\n",
    "                                         params[\"hp_KERNEL_SIZE\"], \n",
    "                                         padding= params[\"hp_PADDING\"], \n",
    "                                         strides=params[\"hp_STRIDES\"], \n",
    "                                         use_bias= params[\"hp_USE_BIAS\"], \n",
    "                                         kernel_regularizer=keras.regularizers.l2(params[\"hp_KERNEL_REGULARIZER\"])))\n",
    "    \n",
    "    if params[\"hp_USE_SECOND_CONV2D\"] is True:\n",
    "        model.add(tf.keras.layers.Conv2D(#params[\"hp_FILTERS\"][nth_layer],\n",
    "                                         filters[nth_layer],\n",
    "                                         params[\"hp_KERNEL_SIZE\"], \n",
    "                                         padding= params[\"hp_PADDING\"], \n",
    "                                         strides=params[\"hp_STRIDES\"], \n",
    "                                         use_bias= params[\"hp_USE_BIAS\"], \n",
    "                                         kernel_regularizer=keras.regularizers.l2(params[\"hp_KERNEL_REGULARIZER\"])))\n",
    "\n",
    "    if params[\"hp_USE_BATCHNORM\"] is True & params[\"hp_BATCHNORM_FIRST\"] is True:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(tf.keras.layers.Activation(params[\"hp_FUNCTION_TYPE\"]))\n",
    "                                         \n",
    "    if params[\"hp_USE_BATCHNORM\"] is True & params[\"hp_BATCHNORM_FIRST\"] is False:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    if params[\"hp_USE_POOLING\"] is True:\n",
    "        model.add(tf.keras.layers.MaxPooling2D(params[\"hp_POOL_SIZE\"], \n",
    "                                               padding= params[\"hp_PADDING\"]))\n",
    "                  \n",
    "    return model\n",
    "\n",
    "def build_between(model, params):\n",
    "    if params[\"hp_USE_BATCHNORM_BTW\"] is True:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "                                         \n",
    "    if params[\"hp_USE_DROPOUT\"] is True:\n",
    "        model.add(tf.keras.layers.Dropout(rate=params[\"hp_DROPOUT_RATE\"]))\n",
    "                                         \n",
    "    return model\n",
    "\n",
    "def build_output(model, params):\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "                                         \n",
    "    if params[\"hp_USE_SECOND_DENSE\"] is True:\n",
    "        model.add(tf.keras.layers.Dense(params[\"hp_DENSE_DIMENSIONALITY\"]))\n",
    "                                         \n",
    "    if params[\"hp_USE_BATCHNORM\"] is True & params[\"hp_BATCHNORM_FIRST\"] is True:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "                                         \n",
    "    model.add(tf.keras.layers.Activation(params[\"hp_LAST_ACTIVATION\"]))\n",
    "                                         \n",
    "    if params[\"hp_USE_BATCHNORM\"] is True & params[\"hp_BATCHNORM_FIRST\"] is False:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "               \n",
    "    if params[\"hp_USE_DROPOUT\"] is True:                                         \n",
    "        model.add(tf.keras.layers.Dropout(rate=params[\"hp_DROPOUT_RATE\"]))\n",
    "                                         \n",
    "    model.add(tf.keras.layers.Dense(7, activation= tf.nn.softmax)) \n",
    "                                         \n",
    "    return model\n",
    "\n",
    "#Initial Hyperparameter Boundaries\n",
    "parameter_space= {\n",
    "    'hp_HIDDEN_LAYERS': hp.quniform('hp_HIDDEN_LAYERS', 2, 5, 1), #scope\n",
    "\n",
    "\n",
    "    #USE OF LAYERS\n",
    "    'hp_USE_BATCHNORM': hp.choice('hp_USE_BATCHNORM', [True\n",
    "                                                       #, False\n",
    "                                                      ]),\n",
    "    'hp_USE_POOLING': hp.choice('hp_USE_POOLING', [True\n",
    "                                                   #, False\n",
    "                                                  ]),\n",
    "    'hp_USE_BATCHNORM_BTW': hp.choice('hp_USE_BATCHNORM_BTW', [True\n",
    "                                                               #, False\n",
    "                                                              ]),\n",
    "    'hp_USE_DROPOUT': hp.choice('hp_USE_DROPOUT', [True\n",
    "                                                   #, False\n",
    "                                                  ]),\n",
    "                                \n",
    "    'hp_USE_SECOND_CONV2D': hp.choice('hp_USE_SECOND_CONV2D', [True\n",
    "                                                               #, False\n",
    "                                                              ]),\n",
    "    'hp_USE_SECOND_DENSE': hp.choice('hp_USE_SECOND_DENSE', [True, False\n",
    "                                                            ]),\n",
    "\n",
    "    #ORDER OF LAYERS\n",
    "    'hp_BATCHNORM_FIRST': hp.choice('hp_BATCHNORM_FIRST', [True\n",
    "                                                           #, False\n",
    "                                                          ]),\n",
    "    \n",
    "    \n",
    "    #CONV-2D-PARAMS\n",
    "    #'hp_FILTERS': hp.choice('hp_FILTERS', \n",
    "    #            [[32, 64, 128, 256, 512], \n",
    "    #              [64, 128, 256, 512, 2014], \n",
    "    #              [128, 256, 512, 1024, 2056]]),\n",
    "    'hp_KERNEL_SIZE': hp.choice('hp_KERNEL_SIZE', \n",
    "                                [\n",
    "                                #(1, 1), \n",
    "                                 (3, 3), \n",
    "                                 #(5, 5)\n",
    "                                ]),\n",
    "    'hp_PADDING': hp.choice('hp_PADDING', ['SAME']),\n",
    "    'hp_STRIDES': hp.choice('hp_STRIDES', [(1, 1)]), #Just one alternative, reducing output with Max2DPooling instead\n",
    "    'hp_USE_BIAS': hp.choice('hp_USE_BIAS', [True, \n",
    "                                             #False\n",
    "                                            ]),\n",
    "    'hp_KERNEL_REGULARIZER': hp.loguniform('hp_KERNEL_REGULARIZER', np.log(0.0001), np.log(0.1)),\n",
    "    \n",
    "    #BATCH-NORM-PARAMS\n",
    "    #-\n",
    "    \n",
    "    #ACTIVATION-PARAMS\n",
    "    'hp_FUNCTION_TYPE': hp.choice('hp_FUNCTION_TYPE', ['relu']),\n",
    "    'hp_LAST_ACTIVATION': hp.choice('hp_LAST_ACTIVATION', ['relu', 'sigmoid']),\n",
    "    \n",
    "    #MAX-POOLING-2D-PARAMS\n",
    "    'hp_POOL_SIZE': hp.choice('hp_POOL_SIZE', [(2, 2)]),\n",
    "    \n",
    "    #DROPOUT-PARAMS\n",
    "    'hp_DROPOUT_RATE': hp.uniform('hp_DROPOUT_RATE', 0.3, 0.7),\n",
    "    \n",
    "    #FLATTEN-PARAMS\n",
    "    \n",
    "    #DENSE-PARAMS\n",
    "    'hp_DENSE_DIMENSIONALITY': hp.choice('hp_DENSE_DIMENSIONALITY', [32, 64, 128]),\n",
    "    \n",
    "    #COMPILE-PARAMS\n",
    "    #'hp_OMPITMIZER': hp.choice('hp_OMPITMIZER', ['rmsprop', 'adam']),\n",
    "    'hp_OMPITMIZER': hp.choice('hp_OMPITMIZER', [tf.keras.optimizers.RMSprop(), tf.keras.optimizers.Adam()]),\n",
    "    \n",
    "    #'hp_EPOCHS': hp.quniform('hp_EPOCHS', 5, 50, 1),\n",
    "    'hp_EPOCHS': hp.choice('hp_EPOCHS', [6]),\n",
    "    'hp_BATCH_SIZE': hp.choice('hp_BATCH_SIZE', [128]),\n",
    "    'hp_LEARNING_RATE': hp.loguniform('hp_LEARNING_RATE', np.log(0.0001), np.log(0.1)),\n",
    "    }\n",
    "\n",
    "opt_hyperparams = optimize(parameter_space)\n",
    "print(opt_hyperparams)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
